1. traditional algorithms들은 multiagent case를 풀기 어려움
   1. non stationary
   2. variance that increases as the number of agents grows
2. other agent의 action policy를 고려하여 이 문제를 해결
3. ensemble of policies를 이용해 더 robust한 MA policy를 만듬


Q-learning => non-stationary and unstable trainig
traditional Actor-Critic methods + Multi agent => very high variance


general purpose multi agent learning algorithm
1. leads to learned policies that only use local information(e.g., thier own observations) at excution time
2. does not assume a differentiable model of the environment dynamics or any particular structure on the communication method between agent
3. can be used to both of cooperative and competitive setting

**centralized training with decentralized execution**->the policies to use extra information to ease training(this information is not used at test time)
q-function cannot contain different information at training and test time -> simple extension of actor-critic policy gradient method to improve the stability of multi-agent policies by training agents with an ensemble of policies

