---
layout: post
title: Jekyll test
categories : [ML,Papers]
---
#### **Paper** ####
Distilling the Knowledge in a Neural Network    
https://arxiv.org/pdf/1503.02531.pdf  

#### **Summary**
가장 좋은 성능 상승 방법은 같은 데이터에 대하여 최대한 많고 다양한 모델들을 학습하여 그들의 prediction을 평균내는 것입니다.
하지만 이런 앙상블을 통한 prediction은 계산적으로 비싸며, 특히 각 모델들이 큰 인공신경망일 때 매우 문제가 됩니다.

앙상블 모델을 학습해서 좋은 성능을 내더라도, 실제 prediction을 수행해야 하는 환경은 매우 제한적일 수 있습니다. 
이럴 경우 제한적인 모델을 사용할수밖에 없는데, 이런 환경에서 앙상블 모델이 학습한 '지식 Knowledge'을 제한적인 모델에
넘겨줄 수 있다면 아주 유용할 것입니다.

Hinton은 Knowledge Distillation(이하 KD) 이라는 방법을 제안합니다.

직역하면 지식 증류. 증류 Distillation은 화학에서 액체 상태의 혼합물을 휘발시켜 분리, 추출하는 방법을 의미합니다. 상당히 은유적인 표현을 사용했습니다. 

앙상블 모델 등 더 복잡하고 거대한 모델이 학습한 것들을 증류하여 지식을 얻고, 그 지식을 더 간단하고 작은 모델에 학습시키면
후자를 그냥 학습하는것보다 더 뛰어난 성능을 보여줄 뿐 아니라 몇몇 재미있는 성질을 관찰할 수 있습니다. 

후속 연구에서들에서는 KD의 대상이 되는 모델을 teacher, 증류된 지식을 받아 교육받는 모델을 student라고 표현합니다.
논문에서 사용되는 표현이 아니지만 직관적이기에 본 포스팅에서는 이 표현을 사용하도록 하겠습니다.

 



