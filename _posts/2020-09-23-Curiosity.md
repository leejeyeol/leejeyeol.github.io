# Curiosity

- Encourage agents to explore the environment more effectively when the rewards are infrequent and sparsely distributed.
ss
- How surprised they are about the outcome of their actions.
  
## Curiosity-driven explorartion
reward : sparse scalar value

We can think of this kind of rewards as being extrinsic because they come from outside the agent.

- If there are extrinsic rewards, then that means there must be intrinsic ones too.
- Intrinsic rewards are generated by the agent itself based on some criteria.
- We want intrinsic rewards which ultimately serve original purpose(maximize extrinsic reward) or that the agent will explore the world more.

Reserchres have put a lot of thought into developing good systems for providing intrinsic reward to agents whitch endow them with similar motivation as we find in nature's agents. One popular approach is to endow the agents with a sense of **curiosity** and to reward it based on how surprised it is by the world around it.

- Human child is curious.. -> this kind of motivation into our agents.

If the agent is rewarded for reaching states which are surprising to it, then it will learn strategies to explore the environment to find more and more surprisinig states. Along the way, the agent will hopefully also discover the extrinsic reward as well.

## Implementations

Deepak Pathak - Curiosity-driven Exploration by Self-supervised Prediction (https://pathak22.github.io/noreward-rl/)


aurhors propose to train two separate NN : a forward and an inverse model.
- The inverse model is trained to take the current and next observation received by the agent, encode them both using a single encoder, and use the result to predict the action that was taken between the occurrence of the two observations.
- The foward model is then trained to take the encoded current observation and action and predict the encoded next observation.
- The difference between the predicted and real encodings is then used as the intrinsic reward, and fed to the agent. (bigger difference means bigger surprise, which in turn means bigger intrinsic reward.)

By using these two models together, the reward not only captures surprising things, but specifically captures surprising things that the agent has control over, based on its actions.
- Thier approach allows an agent trained without any extrinsic reward shaping simply based on its intrinsic reward.

g