theoretical results of MARL
	- Markov games, Stochastic games
    	- goal : Nash equlibrium
	- Extensive form games
  
type
	- fully cooperative
		- common reward function
    	- markov potential game
        	- potential function = accumulated reward
      	- team-average reward
        	- allow agents to get diffent reward
      	- decentralized MARL 
        	- communication protocols. communication efficient MARL algorithm
	- fully competitive
    	- zero sum Markov games
	- mix of two


applications
	- ?


common across the different MARL setting
	- the goals in MARL are multidimensional(multi agent, multi goal) -> chanllenge of dealing with equilibrium point
	- efficiency of communication/ coordination and robustness against potential adversatial agents.
	- each agent becomes non-stationary
	- Combinational nature of MARL- the action space that increase exponentially with the number of agents.
	- who know what? - the information structure. each agent has limited access to the observations of other.
  


Challenges in MARL Theory
- Non-Unique Learning goals
    	- the learning goals of MARL can be vague at times.
    	- The practical MARL agents may not be converge to NE
    	- The goal may be focused on designing the best learning strategy for a given agent and fixed class of the other agent in the game 
- Non-stationarity (Moving target problem)
    	- Multiple agents usually learn conccurrently, causing the environment faced by each individual agent to be non-stationarity.
    	- independent learner (may achieve satisfiable performance in practice)
    	- A surveyof learning in multiagent environments: Dealing with non-stationarity 
    	- why multi-agent domains are non-stationary form? -> consider a simple stochastic game(markov game)
- Scalability Issue	
    	- The combinational nature of MARL
    	- The factorized structures of either the value or reward functions with regard to the action dependence.
- Information Structure
		- Centralized setting (중앙에서 학습 후 뿌림)
		- Decentralized setting with networked agents (각각의 agents가 따로 학습.)
		- Fully decentralized setting
- multi agent credit assignment
- global exploration
- action shadowing <-- ??
- 
=====
MDP -> Partially Observable Markov Games(POMG)



=====

non-stationary

1. ignore
2. forget
3. respond to target models
4. learn models
5. theory of mind


The Environment dynamics -> be known(a priori) or be learned by experience(an interacting many times with the environments)

-> the results break apart when multi agents case.
To tackle this, each agent will need to account for how the other agents are behaving and adapt according to the joint behaviour
-> cause non stationarity problem

Predator(The agent under our control), Prey(The opponent agent)

 Both agents engage in repeated rounds of interactions(possibly infinite), there is no communication and the rewards received depend on the joint action
 The prey has several(possible infinite) strategies at its disposal(ways to select its actions)

- should the predator assume the prey will behave in a cretain way(e.g., minimizing the predator's rewward, enacting their part of the NE or playing as a teammate?)
- should the predator learn to optimise against a single opponent strategy or should it generalise by learning a more robust strategy against a class of possible strategies?
- should the predator assume the prey is modelling the predator's strategy?
- should the predator assume the prey will use a stationary strategy? If not, will the prey change its behaviours slowly or drastically?


transferring single-agent algorithms to the multi agent setting is a natural heuuristic approach(3.3)
the Markov property, denoting a stationary environment, does not hold, thus invalidating guarantees derived for the single agent case.

다른 점 
a = ($a_{i}, a_{-i}$)
the joint policy $\pi(s,a) = mul of \pi_{j}(s,a_{j})$

If the oppenets' are not learning, e.g., they are using a stochastic policy, then the environment is Markovian and single-agent learning algorithms suffice.

Policy generating functions $\tau$, describe how and opponent j obtains its policy $\pi_{j}$
Belief $\beta_{j}$,i.e., a probability distribution over $\tau$, measure an agents's belief about each opponent's reasoning.
Influence function $\theta$ partitions beliefs according to equivalent best responses. (how the belief could be filtered(e.g., to reduce comlexity))
$h_{t}$ = observation history of t obserbations

=====
Game Theory (small problem + multiple agents)
Game Theory Terminology
Player = Agent
Game = Environment
Strategy = Policy
Best Response = Greedy Policy (다른 agent들의 policies가 고정일 경우 )
Utility = Reward
State = (Information) State


Nash Equilibrium is a joint policy such that no player has incentive to deviate unilaterally
- NE always exist in finite game
- computing a NE is PPAD-Complete
  - One solution is to focus on tractable subproblems
  - Another is to compute approximations
- Assumes players are (unbounded) rational
- Assumes Knowledge:
  - Rationality assumption is common knowledge

zero sum, 2player zerosum(minimax)
The sequential setting : Extensive-form game
	- perfect information Games

(finite) Perfect information games : Model
	e.g., episodic MDP
		player identity function (= policy generating function)
		reward per player
=====
Game Theory to RL
e.g., Backward Induction. solving a turn-taking perfect (turn based. 1 player play game at once)
t(s) = player i to play at state s

minimax a.k.a Alpha-Beta, Backward Induction, Retrograde Analysis, etc..
- minimax search

Zero-Sum Markov Games
- Value iteration
    - $V(s) <- minmax E_{a~\pi(s), s'} [r_{1}(s,a,s') + \gammaV_{1}(s')]$


1st era MARL algorithm : Minimax-Q
- 1st era of MARL
	- Friend-or-Foe Q-learning
	- Correlated Q-learning
	- Nash Q-learning
    - Coco-Q 
	- LSPI for Markov Games
	- Evolutionary Game Theory : replicator dynamics
	- WoLF : Win or Learn Fast

rational : oppenents converge to a fixed joint policy
- learning agent converges to a best response of joint policy
Convergent : learner necessarily converges to a fixed policy


2nd era of MARL : Deep learning meets Multiagent RL
- Independent Q-Learning (independent deep q leaning 2015) (independent learner = decentralized learner). non-stationary 등의 문제를 무시하기때문에 잘 안되는 편 단순하지만 문제에 따라 잘 되는 경우도 있음
- Learning to Communicate 
  - (Foester et al. '16, RL based communication, Differentiable communication)
  - (Sukhbaatar et al. '16) CommNet
- Cooperative Multiagent task
  - (Foester et al. '18) Episodic Exploration for DD policies (starcraft)
  - BIC net
- Sequential Social Dilemmas
  - (Leibo et al. '17)
  - (Lerer & Peyskavich '18)
- Centralized Critic Decentralized Actor Approach
  - reduce non stationarity & credit assignment issues using a central critic
  - (Lowe et al. '17) MADDPG
  - (Foester et al. '17) COMA
  - apply to both cooperative and competitive games 
- 3D world MARL
  - Bansel et al. '18 https://arxiv.org/abs/1710.03748
  - Al-Shedivat et al. '17 https://arxiv.org/abs/1710.03641
  - Emergent Coordination Through competitiion (Liu et al. '19) https://arxiv.org/abs/1902.07151 https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer?&
  - Capture-the-Flag (Jaderberg et al. '19) https://deepmind.com/blog/article/capture-the-flag-science
  - alpha star
  - openai five
  - deep MARL survey https://arxiv.org/abs/1810.05587
- Partial Oberservability
  - Fictitious Self-Play, FSP + RL = NFSP (Heinrich et al. '15,16). high expolitability
    - Approximate NE via 2 neural networks : Best Response net(RL), Average policy net(supervised)
  - Quantifying "Joint Policy Correlation" (JPC)
  - Exploitability Descent (Lockhart et al. '19) : extensive form
  - Partially Observable Environment
    -  An information state is a set of histories consistent with an agent's observations.
    -  An information state a corresponds to sequence of observations with respect to the player to act at s.
  - Conterfactual Values
    - what is a counterfactual value?
      - $v_{i}^{c}(\pi, s, a)$
      - The portion of the expected return (under s) from the start state, given that player i plays to reach information state s (then plays a)
    - Conterfactual = ~하지 않았을 때
  - Regret Policy Gradient
    - policy gradient is doing a form of CFR minimization
  - Neural Replicator Dynamics(NeuRD)
  - Counterfactual regret minimization (the advantage values are scaled counterfactual regrets)
    - with update rules for actor-critic algorithms for multiagent partially observable setting : the advantage value are scaled counterfactual regrets -> new convergence properties of independent RL algorithms in zerosum games with impefect information.
  - ->Exploitability Descent
  

● If multi-agent learning is the answer, what is the question?
	○ Shoham et al. ‘06
	○ Hernandez-Leal et al. ‘19
● A comprehensive survey of MARL (Busoniu et al. ‘08)
● Game Theory and Multiagent RL (Nowé et al. ‘12)
● Study of Learning in Multiagent Envs (Hernandez-Leal et al. ‘17)

open spiel  openai gym같은 게임 환경



=========
- Analysis of emergent behaviours
  - to analyze and evaluate DRL algorithms in multiagent environment.
  - emergent behaviours(e.g., cooperative or competitive)
  
- Learning communication (for cooperation)
  - These works explore a sub-area in which agent can share information with communication protocols(direct messages or shared memory)
- Learning cooperation
- Agent modeling agents



convergent result
- minimax Q-learning : in adversarial environments, an optimal play can be guaranteed against an arbitrary opponent
- Nash Q-learning, Friend or Foe Q-learning : in coordination envirionments strong assuptions need be made about other agents to guarnatee convergence to optimal behaviour
- In other type of environments no value-based RL algorithms with guaranteed convergence properties are known.
- Bansal et al. exploration reward(dense. to learn basic control) this type of reward is annealed through time giving more weight to the emergent reward. + oppenent sampling : maintains a pool of older version of the opponent to sample from, in constrast to using the most recent version.
- 
  
Dense reward
- their goal is to provide dense feedback for the learning algorithm to improve sample efficiency
- Ng et al. studied the theoretical conditions under which modification of the reward function of an MDP preserve the optimal policy)
- 